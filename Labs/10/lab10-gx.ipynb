{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import choice\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from collections import namedtuple, defaultdict\n",
    "from itertools import combinations\n",
    "from random import choice, random\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('State', ['x', 'o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGIC = [2, 7, 6, 9, 5, 1, 4, 3, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_board(pos):\n",
    "    \"\"\"Nicely prints the board\"\"\"\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            i = r * 3 + c\n",
    "            if MAGIC[i] in pos.x:\n",
    "                print('X', end='')\n",
    "            elif MAGIC[i] in pos.o:\n",
    "                print('O', end='')\n",
    "            else:\n",
    "                print('.', end='')\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win(elements):\n",
    "    \"\"\"Checks is elements is winning\"\"\"\n",
    "    return any(sum(c) == 15 for c in combinations(elements, 3))\n",
    "\n",
    "def state_value(pos: State):\n",
    "    \"\"\"Evaluate state: +1 first player wins\"\"\"\n",
    "    if win(pos.x):\n",
    "        return 1\n",
    "    elif win(pos.o):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_game():\n",
    "    trajectory = list()\n",
    "    state = State(set(), set())\n",
    "    available = set(range(1, 9+1))\n",
    "    while available:\n",
    "        x = choice(list(available))\n",
    "        state.x.add(x)\n",
    "        trajectory.append(deepcopy(state))\n",
    "        available.remove(x)\n",
    "        if win(state.x) or not available:\n",
    "            break\n",
    "\n",
    "        o = choice(list(available))\n",
    "        state.o.add(o)\n",
    "        trajectory.append(deepcopy(state))\n",
    "        available.remove(o)\n",
    "        if win(state.o):\n",
    "            break\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500000/500000 [01:34<00:00, 5295.82it/s]\n"
     ]
    }
   ],
   "source": [
    "value_dictionary = defaultdict(float)\n",
    "hit_state = defaultdict(int)\n",
    "epsilon = 0.001\n",
    "\n",
    "for steps in tqdm(range(500_000)):\n",
    "    trajectory = random_game()\n",
    "    final_reward = state_value(trajectory[-1])\n",
    "    for state in trajectory:\n",
    "        hashable_state = (frozenset(state.x), frozenset(state.o))\n",
    "        hit_state[hashable_state] += 1\n",
    "        value_dictionary[hashable_state] = value_dictionary[\n",
    "            hashable_state\n",
    "        ] + epsilon * (final_reward - value_dictionary[hashable_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((frozenset({1, 4, 5, 6, 7}), frozenset({2, 3, 8, 9})), 0.9163606208461627),\n",
       " ((frozenset({1, 3, 6, 8, 9}), frozenset({2, 4, 5, 7})), 0.9156884847308895),\n",
       " ((frozenset({1, 2, 4, 5, 8}), frozenset({3, 6, 7, 9})), 0.913206393246716),\n",
       " ((frozenset({2, 3, 5, 8, 9}), frozenset({1, 4, 6, 7})), 0.9131195127594756),\n",
       " ((frozenset({1, 2, 4, 6, 7}), frozenset({3, 5, 8, 9})), 0.9122459102842707),\n",
       " ((frozenset({2, 4, 5, 7, 8}), frozenset({1, 3, 6, 9})), 0.9117175379847842),\n",
       " ((frozenset({4, 5, 6, 7, 9}), frozenset({1, 2, 3, 8})), 0.9116291671519361),\n",
       " ((frozenset({1, 4, 5, 7, 9}), frozenset({2, 3, 6, 8})), 0.9110080894969314),\n",
       " ((frozenset({1, 5, 7, 8, 9}), frozenset({2, 3, 4, 6})), 0.9109190085054368),\n",
       " ((frozenset({3, 4, 6, 8, 9}), frozenset({1, 2, 5, 7})), 0.9108298383437806)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(value_dictionary.items(), key=lambda e: e[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5477"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hit_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one agent randome and the other q-dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(epsilon, state, value_dict):\n",
    "    \"\"\"Epsilon-greedy exploration strategy\"\"\"\n",
    "    hashable_state = (frozenset(state.x), frozenset(state.o))\n",
    "    available_actions = list(set(range(1, 9+1)) - set(state.x) - set(state.o))\n",
    "\n",
    "    if not available_actions or random() < epsilon:\n",
    "        # Explore: choose a random action\n",
    "        if available_actions:\n",
    "            return choice(available_actions)\n",
    "        else:\n",
    "            return choice(list(range(1, 9+1)))  # Choose any action if none are available\n",
    "    else:\n",
    "        # Exploit: choose the action with the highest Q-value\n",
    "        q_values = {action: value_dict[hashable_state + (action,)] for action in available_actions}\n",
    "        return max(q_values, key=q_values.get)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_train(value_dict, hit_state, epsilon, alpha, gamma, num_episodes):\n",
    "    \"\"\"Q-learning training\"\"\"\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        state = State(set(), set())\n",
    "        available = set(range(1, 9+1))\n",
    "        while available:\n",
    "            action = epsilon_greedy(epsilon, state, value_dict)\n",
    "            state_prime = deepcopy(state)\n",
    "            state_prime.x.add(action)\n",
    "            if win(state_prime.x) or not available:\n",
    "                reward = state_value(state_prime)\n",
    "                update_q_value(state, action, reward, state_prime, value_dict, alpha, gamma)\n",
    "                break\n",
    "\n",
    "            o = choice(list(available))\n",
    "            state_prime.o.add(o)\n",
    "            if win(state_prime.o):\n",
    "                reward = state_value(state_prime)\n",
    "                update_q_value(state, action, reward, state_prime, value_dict, alpha, gamma)\n",
    "                break\n",
    "\n",
    "            update_q_value(state, action, 0, state_prime, value_dict, alpha, gamma)\n",
    "            state = deepcopy(state_prime)\n",
    "\n",
    "            # Ensure that action is present in the available set before removing it\n",
    "            if action in available:\n",
    "                available.remove(action)\n",
    "\n",
    "        # # Print the Q-values periodically to observe the learning progress\n",
    "        # if episode % 1000 == 0:\n",
    "        #     print(f\"Episode: {episode}, Q-Values: {sorted(value_dict.items(), key=lambda e: e[1], reverse=True)[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_value(state, action, reward, state_prime, value_dict, alpha, gamma):\n",
    "    \"\"\"Update Q-value based on the Q-learning update rule\"\"\"\n",
    "    hashable_state = (frozenset(state.x), frozenset(state.o))\n",
    "    hashable_state_prime = (frozenset(state_prime.x), frozenset(state_prime.o))\n",
    "    \n",
    "    available_actions_prime = list(set(range(1, 9+1)) - set(state_prime.x) - set(state_prime.o))\n",
    "    \n",
    "    if available_actions_prime:\n",
    "        max_q_value_prime = max(value_dict[hashable_state_prime + (a,)] for a in available_actions_prime)\n",
    "    else:\n",
    "        max_q_value_prime = 0  # If no valid actions available, set the max Q-value to 0\n",
    "    \n",
    "    value_dict[hashable_state + (action,)] += alpha * (\n",
    "            reward + gamma * max_q_value_prime -\n",
    "            value_dict[hashable_state + (action,)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-learning hyperparameters\n",
    "alpha = 0.1  # learning rate\n",
    "gamma = 0.9  # discount factor\n",
    "num_episodes = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500000/500000 [01:35<00:00, 5247.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-Values:\n",
      "[((frozenset({8, 1, 2, 4}), frozenset({9, 3, 5}), 6), 0.9999999999999996), ((frozenset({1, 2, 4}), frozenset({8, 3, 6}), 9), 0.9999999999999996), ((frozenset({8, 1, 2, 3}), frozenset({9, 5, 6}), 4), 0.9999999999999996), ((frozenset({1, 2, 4}), frozenset({1, 3, 7}), 9), 0.9999999999999996), ((frozenset({1, 2, 4}), frozenset({3, 5, 6}), 9), 0.9999999999999996), ((frozenset({1, 2, 4}), frozenset({8, 3, 7}), 9), 0.9999999999999996), ((frozenset({1, 2, 4}), frozenset({8, 5, 7}), 9), 0.9999999999999996), ((frozenset({1, 2, 4, 5}), frozenset({8, 9, 3}), 6), 0.9999999999999996), ((frozenset({1, 2, 4}), frozenset({8, 5, 6}), 9), 0.9999999999999996), ((frozenset({1, 2, 4}), frozenset({1, 3, 5}), 9), 0.9999999999999996)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize Q-values and train the agent\n",
    "value_dictionary = defaultdict(float)\n",
    "hit_state = defaultdict(int)\n",
    "epsilon = 0.1  # exploration-exploitation trade-off\n",
    "q_learning_train(value_dictionary, hit_state, epsilon, alpha, gamma, num_episodes)\n",
    "\n",
    "# After the training loop, print the final Q-values\n",
    "print(\"Final Q-Values:\")\n",
    "print(sorted(value_dictionary.items(), key=lambda e: e[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(value_dict):\n",
    "    state = State(set(), set())\n",
    "    available = set(range(1, 9+1))\n",
    "    while available:\n",
    "        print_board(state)\n",
    "        action = epsilon_greedy(0, state, value_dict)  # Choose greedy action (no exploration)\n",
    "        state.x.add(action)\n",
    "        if win(state.x) or not available:\n",
    "            print(\"Agent wins!\")\n",
    "            break\n",
    "\n",
    "        o = choice(list(available))\n",
    "        state.o.add(o)\n",
    "        if win(state.o):\n",
    "            print(\"Opponent wins!\")\n",
    "            break\n",
    "\n",
    "        # Ensure that action is present in the available set before removing it\n",
    "        if action in available:\n",
    "            available.remove(action)\n",
    "\n",
    "    # Print the final board state\n",
    "    print(\"Final Board State:\")\n",
    "    print_board(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "...\n",
      "...\n",
      "\n",
      "...\n",
      "..X\n",
      "..O\n",
      "\n",
      "...\n",
      ".XX\n",
      "..O\n",
      "\n",
      "Agent wins!\n",
      "Final Board State:\n",
      "...\n",
      "XXX\n",
      "..O\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_agent(value_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "both players learn and use q-dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(epsilon, state, value_dict):\n",
    "    \"\"\"Epsilon-greedy exploration strategy\"\"\"\n",
    "    hashable_state = (frozenset(state.x), frozenset(state.o))\n",
    "    available_actions = list(set(range(1, 9+1)) - set(state.x) - set(state.o))\n",
    "\n",
    "    if not available_actions or random() < epsilon:\n",
    "        # Explore: choose a random action\n",
    "        if available_actions:\n",
    "            return choice(available_actions)\n",
    "        else:\n",
    "            return choice(list(range(1, 9+1)))  # Choose any action if none are available\n",
    "    else:\n",
    "        # Exploit: choose the action with the highest Q-value\n",
    "        q_values = {action: value_dict[hashable_state + (action,)] for action in available_actions}\n",
    "        return max(q_values, key=q_values.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_value(state, action, reward, state_prime, value_dict, alpha, gamma):\n",
    "    \"\"\"Update Q-value based on the Q-learning update rule\"\"\"\n",
    "    hashable_state = (frozenset(state.x), frozenset(state.o))\n",
    "    hashable_state_prime = (frozenset(state_prime.x), frozenset(state_prime.o))\n",
    "    \n",
    "    available_actions_prime = list(set(range(1, 9+1)) - set(state_prime.x) - set(state_prime.o))\n",
    "    \n",
    "    if available_actions_prime:\n",
    "        max_q_value_prime = max(value_dict[hashable_state_prime + (a,)] for a in available_actions_prime)\n",
    "    else:\n",
    "        max_q_value_prime = 0  # If no valid actions available, set the max Q-value to 0\n",
    "    \n",
    "    value_dict[hashable_state + (action,)] += alpha * (\n",
    "            reward + gamma * max_q_value_prime -\n",
    "            value_dict[hashable_state + (action,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_train(agent_value_dict, opponent_value_dict, epsilon, alpha, gamma, num_episodes):\n",
    "    \"\"\"Q-learning training for both agent and opponent\"\"\"\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        agent_state = State(set(), set())\n",
    "        opponent_state = State(set(), set())\n",
    "        agent_available = set(range(1, 9+1))\n",
    "        opponent_available = set(range(1, 9+1))\n",
    "        \n",
    "        while agent_available:\n",
    "            # Agent's turn\n",
    "            agent_action = epsilon_greedy(epsilon, agent_state, agent_value_dict)\n",
    "            agent_state_prime = deepcopy(agent_state)\n",
    "            agent_state_prime.x.add(agent_action)\n",
    "            \n",
    "            if win(agent_state_prime.x) or not agent_available:\n",
    "                reward = state_value(agent_state_prime)\n",
    "                update_q_value(agent_state, agent_action, reward, agent_state_prime, agent_value_dict, alpha, gamma)\n",
    "                break\n",
    "\n",
    "            # Opponent's turn\n",
    "            opponent_action = epsilon_greedy(epsilon, opponent_state, opponent_value_dict)\n",
    "            opponent_state_prime = deepcopy(opponent_state)\n",
    "            opponent_state_prime.o.add(opponent_action)\n",
    "\n",
    "            if win(opponent_state_prime.o) or not opponent_available:\n",
    "                reward = state_value(opponent_state_prime)\n",
    "                update_q_value(opponent_state, opponent_action, reward, opponent_state_prime, opponent_value_dict, alpha, gamma)\n",
    "                break\n",
    "\n",
    "            update_q_value(agent_state, agent_action, 0, agent_state_prime, agent_value_dict, alpha, gamma)\n",
    "            agent_state = deepcopy(agent_state_prime)\n",
    "            agent_available.remove(agent_action)\n",
    "\n",
    "            update_q_value(opponent_state, opponent_action, 0, opponent_state_prime, opponent_value_dict, alpha, gamma)\n",
    "            opponent_state = deepcopy(opponent_state_prime)\n",
    "            opponent_available.remove(opponent_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-values for both agent and opponent\n",
    "agent_value_dictionary = defaultdict(float)\n",
    "opponent_value_dictionary = defaultdict(float)\n",
    "agent_hit_state = defaultdict(int)\n",
    "opponent_hit_state = defaultdict(int)\n",
    "\n",
    "# Placeholder for other initializations if you have any\n",
    "\n",
    "# Q-learning hyperparameters\n",
    "alpha = 0.1  # learning rate\n",
    "gamma = 0.9  # discount factor\n",
    "num_episodes = 500000\n",
    "\n",
    "# Exploration-exploitation trade-off\n",
    "epsilon = 0.1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500000/500000 [02:12<00:00, 3760.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-Values for Agent:\n",
      "[((frozenset({1, 2, 4}), frozenset(), 9), 0.9999999999999996), ((frozenset({1, 2, 3, 4}), frozenset(), 9), 0.9999999999999996), ((frozenset({1, 2, 3, 4, 5}), frozenset(), 8), 0.9999999999999996), ((frozenset({1, 6}), frozenset(), 8), 0.9999999999999996), ((frozenset({1, 2, 6}), frozenset(), 7), 0.9999999999999996), ((frozenset({1, 2, 6}), frozenset(), 8), 0.9999999999999996), ((frozenset({1, 2, 3, 4, 6}), frozenset(), 8), 0.9999999999999996), ((frozenset({1, 2, 3, 7}), frozenset(), 6), 0.9999999999999996), ((frozenset({1, 2, 3, 4, 7}), frozenset(), 8), 0.9999999999999996), ((frozenset({1, 2, 3, 5}), frozenset(), 7), 0.9999999999999996)]\n",
      "Final Q-Values for Opponent:\n",
      "[((frozenset(), frozenset(), 1), 0.0), ((frozenset(), frozenset(), 2), 0.0), ((frozenset(), frozenset(), 3), 0.0), ((frozenset(), frozenset(), 4), 0.0), ((frozenset(), frozenset(), 5), 0.0), ((frozenset(), frozenset(), 6), 0.0), ((frozenset(), frozenset(), 7), 0.0), ((frozenset(), frozenset(), 8), 0.0), ((frozenset(), frozenset(), 9), 0.0), ((frozenset(), frozenset({1}), 2), 0.0)]\n"
     ]
    }
   ],
   "source": [
    "# Train both agent and opponent\n",
    "q_learning_train(agent_value_dictionary, opponent_value_dictionary, epsilon, alpha, gamma, num_episodes)\n",
    "\n",
    "# After the training loop, print the final Q-values for both agent and opponent\n",
    "print(\"Final Q-Values for Agent:\")\n",
    "print(sorted(agent_value_dictionary.items(), key=lambda e: e[1], reverse=True)[:10])\n",
    "\n",
    "print(\"Final Q-Values for Opponent:\")\n",
    "print(sorted(opponent_value_dictionary.items(), key=lambda e: e[1], reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's turn:\n",
      "...\n",
      "...\n",
      "...\n",
      "\n",
      "Opponent's turn:\n",
      "...\n",
      "..X\n",
      "...\n",
      "\n",
      "Agent's turn:\n",
      "O..\n",
      "..X\n",
      "...\n",
      "\n",
      "Opponent's turn:\n",
      "O..\n",
      "..X\n",
      ".X.\n",
      "\n",
      "Agent's turn:\n",
      "O..\n",
      "..X\n",
      "OX.\n",
      "\n",
      "Opponent's turn:\n",
      "O..\n",
      ".XX\n",
      "OX.\n",
      "\n",
      "Agent's turn:\n",
      "O..\n",
      ".XX\n",
      "OXO\n",
      "\n",
      "Agent wins!\n",
      "Final Board State:\n",
      "O..\n",
      "XXX\n",
      "OXO\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ... (previous code)\n",
    "\n",
    "# Test the learned agent against a random opponent\n",
    "def test_agent_vs_opponent(agent_value_dict, opponent_value_dict):\n",
    "    agent_state = State(set(), set())\n",
    "    opponent_state = State(set(), set())\n",
    "    game_state = State(set(), set())  # Shared game state\n",
    "    agent_available = set(range(1, 9+1))\n",
    "    opponent_available = set(range(1, 9+1))\n",
    "    \n",
    "    while agent_available:\n",
    "        print(\"Agent's turn:\")\n",
    "        print_board(game_state)\n",
    "        agent_action = epsilon_greedy(0, game_state, agent_value_dict)  # Choose greedy action (no exploration)\n",
    "        game_state.x.add(agent_action)\n",
    "        \n",
    "        if win(game_state.x) or not agent_available:\n",
    "            print(\"Agent wins!\")\n",
    "            break\n",
    "\n",
    "        print(\"Opponent's turn:\")\n",
    "        print_board(game_state)\n",
    "        opponent_action = epsilon_greedy(0, game_state, opponent_value_dict)  # Choose greedy action (no exploration)\n",
    "        game_state.o.add(opponent_action)\n",
    "\n",
    "        if win(game_state.o) or not opponent_available:\n",
    "            print(\"Opponent wins!\")\n",
    "            break\n",
    "\n",
    "        # Ensure that action is present in the available set before removing it\n",
    "        if agent_action in agent_available:\n",
    "            agent_available.remove(agent_action)\n",
    "\n",
    "        if opponent_action in opponent_available:\n",
    "            opponent_available.remove(opponent_action)\n",
    "\n",
    "    # Print the final board state\n",
    "    print(\"Final Board State:\")\n",
    "    print_board(game_state)\n",
    "\n",
    "# Test the learned agent against a random opponent\n",
    "test_agent_vs_opponent(agent_value_dictionary, opponent_value_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-P-7LqQ3C-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
